{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers import Dropout # dropout crew 4 lyf\n",
    "from IPython.display import clear_output # clear plot after each epoch\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt # fancy plots\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime # so we know how much time we've wasted\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained_models/best_model.h5\n",
      "corpus length : 126852\n",
      "unique chars  : 38\n",
      "total patterns: 42271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## load a model for testing\n",
    "model_path = os.path.join('trained_models','best_model.h5') # load external file\n",
    "model = load_model(model_path)\n",
    "print('Loaded', model_path)\n",
    "\n",
    "## load the corpus\n",
    "path = os.path.join('data','snoop_dups.txt') # path to the corpus\n",
    "\n",
    "with open(path, encoding='utf-8', errors='ignore') as f: # errors=ignore strips non utf-8 chars\n",
    "    text = f.read().lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print('corpus length :', len(text))\n",
    "print('unique chars  :', len(chars))\n",
    "print('total patterns:', len(sentences))\n",
    "print('') # empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** seed: < becomes your every thoughtkeep your eye >\n",
      "\n",
      " becomes your every thoughtkeep your eye on your playa the everything\n",
      "i would you get now check the might niggaz droppydy nowgama cans\n",
      "in alo be comi bitch sick and miss of the wast for yey coust him hear hack\n",
      "cause im lave it to you mirh sump mire\n",
      "\n",
      "verse twe see doggystyle\n",
      "on a ond ene koockst ffrered in hoints\n",
      "you cant ge to the g to shool\n",
      "\n",
      "hers i was like im the dont lic me so want hare my modoon dogg been it and pice\n",
      "you went all ya\n"
     ]
    }
   ],
   "source": [
    "## generate output\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "\n",
    "temperature = 0.5 # 0.5-2ish, higher numbers give more unique output\n",
    "maxChars = 400 # length of new output\n",
    "\n",
    "seed = sentence.replace('\\n','') # cut newlines\n",
    "seed = seed.replace('\\t','')\n",
    "\n",
    "print('*** seed: <', seed ,'>') # print the seed (text input)\n",
    "print('\\n',seed, end = '')\n",
    "\n",
    "# make some words\n",
    "for i in range(maxChars):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
