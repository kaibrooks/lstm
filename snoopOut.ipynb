{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers import Dropout # dropout crew 4 lyf\n",
    "from IPython.display import clear_output # clear plot after each epoch\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt # fancy plots\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime # so we know how much time we've wasted\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained_models/best_model.h5\n",
      "corpus length : 126852\n",
      "unique chars  : 38\n",
      "total patterns: 42271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## load a model for testing\n",
    "model_path = os.path.join('trained_models','best_model.h5') # load external file\n",
    "model = load_model(model_path)\n",
    "print('Loaded', model_path)\n",
    "\n",
    "## load the corpus\n",
    "path = os.path.join('data','snoop_dups.txt') # path to the corpus\n",
    "\n",
    "with open(path, encoding='utf-8', errors='ignore') as f: # errors=ignore strips non utf-8 chars\n",
    "    text = f.read().lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print('corpus length :', len(text))\n",
    "print('unique chars  :', len(chars))\n",
    "print('total patterns:', len(sentences))\n",
    "print('') # empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** seed: < hots for all the plans and plotsi gots  >\n",
      "\n",
      " hots for all the plans and plotsi gots to be thin i me doo\n",
      "about this ounce come and take you aint was how you got doggystyle i keep is when a might oot funkin what do but i got my hand\n",
      "and all that a bitch just a niggaz on til i dont quit\n",
      "\n",
      "and wa fuck and see im a bitch betary plice shroke\n",
      "\n",
      "rakin money pather me to doght is a putty come\n",
      "1unch it all back blocks of cos im the doggy dogg yay you wanna have yo whe hereeve in the cleck th\n"
     ]
    }
   ],
   "source": [
    "## generate output\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "\n",
    "temperature = 0.5 # 0.5-2ish, higher numbers give more unique output\n",
    "maxChars = 400 # length of new output\n",
    "\n",
    "seed = sentence.replace('\\n','') # cut newlines\n",
    "seed = seed.replace('\\t','')\n",
    "\n",
    "print('*** seed: <', seed ,'>') # print the seed (text input)\n",
    "print('\\n',seed, end = '')\n",
    "\n",
    "# make some words\n",
    "for i in range(maxChars):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
